# -*- coding: utf-8 -*-
"""YOLO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SDVT0wpcJBcOxj74S-EpDe6lSgmcN2ai
"""

from google.colab import drive
drive.mount('/content/drive/')

import pandas as pd
import numpy as np
import cv2
import matplotlib.pyplot as plt
from glob import glob
import IPython.display as ipd
from tqdm.notebook import tqdm
import subprocess

"""### Process Video"""

input_file = '/content/drive/MyDrive/UH Spring 2024/COSC 4331 Real-Time Systems & Embedded Programming/Project/Object-Detection/src/data/Video1.mov'
subprocess.run(['ffmpeg',
                '-i',
                input_file,
                '-qscale',
                '0',
                'Video1.mp4',
                '-loglevel',
                'quiet']
              )

ipd.Video('/content/Video1.mp4', embed=True, width=700)

input_file = '/content/drive/MyDrive/UH Spring 2024/COSC 4331 Real-Time Systems & Embedded Programming/Project/Object-Detection/src/data/Video2.mov'
subprocess.run(['ffmpeg',
                '-i',
                input_file,
                '-qscale',
                '0',
                'Video2.mp4',
                '-loglevel',
                'quiet']
              )

ipd.Video('/content/Video2.mp4', embed=True, width=700)

input_file = '/content/drive/MyDrive/UH Spring 2024/COSC 4331 Real-Time Systems & Embedded Programming/Project/Object-Detection/src/data/Video3.mov'
subprocess.run(['ffmpeg',
                '-i',
                input_file,
                '-qscale',
                '0',
                'Video3.mp4',
                '-loglevel',
                'quiet']
              )

ipd.Video('/content/Video3.mp4', embed=True, width=700)

input_file = '/content/drive/MyDrive/UH Spring 2024/COSC 4331 Real-Time Systems & Embedded Programming/Project/Object-Detection/src/data/0000f77c-6257be58.mov'
# subprocess.run(['ffmpeg', '-i', input_file, '-qscale', '0', '0000f77c-6257be58.mp4', '-loglevel', 'quiet'])
subprocess.run(['ffmpeg',
                '-i',
                input_file,
                '-qscale',
                '0',
                '0000f77c-6257be58.mp4',
                '-loglevel',
                'quiet']
              )

!ls -GFlash --color

"""### Display Video"""

ipd.Video('/content/Video1.mp4', embed=True, width=700)

"""### Open Video & Read Metadata"""

cap = cv2.VideoCapture('/content/Video1.mp4')

num_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)
print('Total Number of Frames:', num_frames)

height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)
width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)
print(f'Height: {height}, Width: {width}')

frames_per_second = cap.get(cv2.CAP_PROP_FPS)
print(f'FPS: {frames_per_second:0.2f}')

cap.release()

"""### Pull Images From Video"""

cap = cv2.VideoCapture('/content/Video1.mp4')
ret, img = cap.read()
print(f'Returned {ret} and img of shape {img.shape}')

plt.imshow(img)

"""### Add RGB to Image"""

def display_cv2_img(img, figsize=(10, 10)):
    img_ = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    fig, ax = plt.subplots(figsize=figsize)
    ax.imshow(img_)
    ax.axis('off')

display_cv2_img(img)

cap.release()

"""### Iterate Over Frames"""

fig, axs = plt.subplots(5, 5, figsize=(30, 20))
axs = axs.flatten()

cap = cv2.VideoCapture('/content/Video1.mp4')
num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

img_idx = 0

for frame in range(num_frames):
    ret, img = cap.read()
    if ret == False:
        break
    if frame % 100 == 0:
        axs[img_idx].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
        axs[img_idx].set_title(f'Frame: {frame}')
        axs[img_idx].axis('off')
        img_idx += 1

plt.tight_layout()
plt.show()
cap.release()

"""### Classify Video Images"""

labels = pd.read_csv('/content/drive/MyDrive/UH Spring 2024/COSC 4331 Real-Time Systems & Embedded Programming/Project/Object-Detection/src/data/mot_labels.csv', low_memory=False)

video_labels = (labels.query('videoName == "Video1"').reset_index(drop=True).copy())

video_labels['video_frame'] = (video_labels['frameIndex'] * 11.9).round().astype('int')

video_labels['category'].value_counts()

cap = cv2.VideoCapture('/content/Video1.mp4')
num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

img_idx = 0

for frame in range(num_frames):
    ret, img = cap.read()
    if ret == False:
        break
    if frame == 400:
        break

cap.release()

display_cv2_img(img)

img_example = img.copy()
frame_labels = video_labels.query('video_frame == 400')

for i, d in frame_labels.iterrows():
    pt1 = int(d['box2d.x1']), int(d['box2d.y1'])
    pt2 = int(d['box2d.x2']), int(d['box2d.y2'])
    cv2.rectangle(img_example, pt1, pt2, (0, 0, 255), 3)

display_cv2_img(img_example)

color_map = {
      "car": (0, 0, 255),
      "truck": (0, 0, 100),
      "pedestrian": (255, 0, 0),
      "other vehicle": (0, 0, 150),
      "rider": (200, 0, 0),
      "bicycle": (0, 255, 0),
      "other person": (200, 0, 0),
      "trailer": (0, 150, 150),
      "motorcycle": (0, 150, 0),
      "bus": (0, 0, 100),
}

img_example = img.copy()
frame_labels = video_labels.query('video_frame == 400')

for i, d in frame_labels.iterrows():
    pt1 = int(d['box2d.x1']), int(d['box2d.y1'])
    pt2 = int(d['box2d.x2']), int(d['box2d.y2'])
    color = color_map[d['category']]
    cv2.rectangle(img_example, pt1, pt2, color, 3)

display_cv2_img(img_example)

frame_labels = video_labels.query('video_frame == @frame')
font = cv2.FONT_HERSHEY_TRIPLEX
img_example = img.copy()

for i, d in frame_labels.iterrows():
    pt1 = int(d['box2d.x1']), int(d['box2d.y1'])
    pt2 = int(d['box2d.x2']), int(d['box2d.y2'])
    color = color_map[d['category']]
    img_example = cv2.rectangle(img_example, pt1, pt2, color, 3)
    pt_text = int(d['box2d.x1'] + 5), int(d['box2d.y1'] + 10)
    img_example = cv2.putText(img_example, d['category'], pt_text, font, 0.5, color)

display_cv2_img(img_example)
cap.release()

"""### Label Video"""

def add_annotations(img, frame, video_lables):
    max_frame = video_labels.query('video_frame <= @frame')['video_frame'].max()
    frame_labels = video_labels.query('video_frame == @max_frame')
    for i, d in frame_labels.iterrows():
      pt1 = int(d['box2d.x1']), int(d['box2d.y1'])
      pt2 = int(d['box2d.x2']), int(d['box2d.y2'])
      color = color_map[d['category']]
      img = cv2.rectangle(img, pt1, pt2, color, 3)
    return img

!rm -r out_test.mp4

VIDEO_CODEC = 'mp4v'
fps = 59.94
width = 1280
height = 720
out = cv2.VideoWriter('out_test.mp4', cv2.VideoWriter_fourcc(*VIDEO_CODEC), fps, (width, height))

cap = cv2.VideoCapture('/content/0000f77c-6257be58.mp4')
num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

for frame in tqdm(range(num_frames), total = num_frames):
    ret, img = cap.read()
    if ret == False:
        break
    img = add_annotations(img, frame, video_labels)
    out.write(img)
out.release()
cap.release()

!ls -GFlash -color

"""### Convert To MP4 & View"""

tmp_output_path = 'out_test.mp4'
output_path = 'out_test_compressed.mp4'
subprocess.run(['ffmpeg',
                '-i',
                tmp_output_path,
                '-crf',
                '18',
                '-preset',
                'veryfast',
                '-vcodec',
                'libx264',
                output_path,
                '-loglevel',
                'quiet']
              )

ipd.Video('/content/out_test_compressed.mp4', embed=True, width=700)

"""# YOLO"""

pip install opencv-python

import cv2

!nvidia-smi

from psutil import *
# This code will return the number of CPU
print("Number of CPU: ", cpu_count())
# This code will return the CPU info
!cat /proc/cpuinfo

!rm -fr darknet/

!git clone https://github.com/AlexeyAB/darknet/

# Commented out IPython magic to ensure Python compatibility.
# %cd darknet/
with open('Makefile', 'r') as file:
    filedata = file.read()

# Replace GPU and OPENCV flags
filedata = filedata.replace('GPU=0', 'GPU=1')
filedata = filedata.replace('OPENCV=0', 'OPENCV=1')

# Write the modified Makefile back
with open('Makefile', 'w') as file:
    file.write(filedata)

import urllib.request

# Specify the URL of the YOLOv3 weights file
url = 'https://pjreddie.com/media/files/yolov3.weights'

# Download the file and save it locally
urllib.request.urlretrieve(url, 'yolov3.weights')

import os

# Specify the path to the darknet executable
darknet_path = './darknet'

# Set the executable permission
os.chmod(darknet_path, 0o755)  # 0o755 corresponds to rwxr-xr-x permission

# Commented out IPython magic to ensure Python compatibility.
# %cd darknet/
!sed -i 's/GPU=0/GPU=1/g' Makefile
!sed -i 's/OPENCV=0/OPENCV=1/g' Makefile
!make

"""### Get YOLO Weights"""

!wget https://pjreddie.com/media/files/yolov3.weights
!chmod a+x ./darknet

!apt install ffmpeg libopencv-dev libgtk-3-dev python3-numpy libjpeg-dev libtiff5-dev libavcodec-dev libavformat-dev libswscale-dev libxine2-dev libgstreamer1.0-dev libgstreamer-plugins-base1.0-dev libv4l-dev libtbb-dev qtbase5-dev libfaac-dev libmp3lame-dev libopencore-amrnb-dev libopencore-amrwb-dev libtheora-dev libvorbis-dev libxvidcore-dev x264 v4l-utils unzip

from google.colab import files
uploaded = files.upload()

!./darknet detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show ../Video1.mp4 -i 0 -out_filename Video1.mp4 -thresh 0.7

!./darknet detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show ../Video2.mp4 -i 0 -out_filename Video2.mp4 -thresh 0.7

!./darknet detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show ../Video3.mp4 -i 0 -out_filename Video3.mp4 -thresh 0.7

files.download('Video1.mp4')

files.download('Video2.mp4')

files.download('Video3.mp4')

"""# Track Real-Time"""

pip install sort-track

from sort.tracker import SortTracker
import time

tracker = SortTracker()

cap = cv2.VideoCapture('/content/Video1.mp4')

# Loop through video frames
while True:
    ret, frame = cap.read()
    if not ret:
        break

    # Perform object detection with YOLO
    detections = yolo.detect_objects(frame)

    # Associate detections with previous tracks using SORT
    tracked_objects = tracker.update(detections)

    # Update object tracks with start and end times
    for obj_id, bbox in tracked_objects.items():
        if obj_id not in object_tracks:
            # New object track detected
            object_tracks[obj_id] = {'start_time': time.time(), 'end_time': None}
        else:
            # Update end time for existing object track
            object_tracks[obj_id]['end_time'] = time.time()

    # Display annotated frame (optional)
    for obj_id, bbox in tracked_objects.items():
        x1, y1, x2, y2 = bbox
        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
        cv2.putText(frame, f"ID: {obj_id}", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    cv2.imshow('Object Tracking', frame)

    # Press 'q' to quit
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release video capture and close windows
cap.release()
cv2.destroyAllWindows()

import matplotlib.pyplot as plt

# Extract confidence scores from detections
confidence_scores = [detection['confidence'] for detection in detections]

# Plot histogram of confidence scores
plt.figure(figsize=(8, 6))
plt.hist(confidence_scores, bins=20, color='skyblue', edgecolor='black')
plt.xlabel('Confidence Score')
plt.ylabel('Frequency')
plt.title('Histogram of Confidence Scores')
plt.grid(True)
plt.show()

import subprocess

# Define the command as a list of strings
command = [
    "./darknet",
    "detector",
    "demo",
    "cfg/coco.data",
    "cfg/yolov3.cfg",
    "yolov3.weights",
    "-dont_show",
    "/content/Video1.mp4",
    "-i",
    "0",
    "-out_filename",
    "output1.avi",
    "-thresh",
    "0.7"
]

# Execute the command and capture the output
process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

# Wait for the process to complete
stdout, stderr = process.communicate()

# Print the output
print("Output:")
print(stdout.decode('utf-8'))

# Print any errors
if stderr:
    print("Errors:")
    print(stderr.decode('utf-8'))

import subprocess

# Define the command as a list of strings
command = [
    "./darknet",
    "detector",
    "demo",
    "cfg/coco.data",
    "cfg/yolov3.cfg",
    "yolov3.weights",
    "-dont_show",
    "/content/Video3.mp4",
    "-i",
    "0",
    "-out_filename",
    "output1.avi",
    "-thresh",
    "0.7"
]

# Execute the command and capture the output
process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

# Wait for the process to complete
stdout, stderr = process.communicate()

# Print the output
print("Output:")
print(stdout.decode('utf-8'))

# Print any errors
if stderr:
    print("Errors:")
    print(stderr.decode('utf-8'))